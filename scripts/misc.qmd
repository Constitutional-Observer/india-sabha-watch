
# download debate english only docs

```{python}
import pandas as pd
import bs4 as soup
import requests
from io import StringIO
from tqdm import tqdm
import time

from concurrent.futures import ThreadPoolExecutor

Englishlink = "https://eparlib.nic.in/simple-search?page-token=63dbc7fa77d0&page-token-value=a982e808be30077f19f95caabd94f345&location=123456789%2F2963706&query=&rpp=2500&sort_by=dc.date_dt&order=desc&filter_field_1=loksabhanumber&filter_type_1=equals&filter_value_1=17&submit_filter_remove_1=X"

OriginalLink = "https://eparlib.nic.in/handle/123456789/7/simple-search?query=&filter_field_1=title&filter_type_1=equals&filter_value_1=Lok+Sabha+Debates&sort_by=dc.date_dt&order=desc&rpp=7000&etal=0&start=20"


def download_all_parent_html(link):
    for i in range(99740, 129136, 20):
        tlink = link + str(i)
        # tempdf = get_full_table(tlink)
        # download all html Files

        file = requests.get(tlink).content

        with open("../data/sabha/htmlbkup/part2/" + str(i) + ".html", "wb") as f:
            f.write(file)

        ## sleep randomly between 1 and 2 seconds
        time.sleep(random.randint(1, 5))

def download_all_meta_html(df):
    for i, x in enumerate(tqdm(df["Meta_Link"])):
        print(x)
        file = requests.get(x).content
        with open("../data/sabha/htmlbkup/part2/meta/" + str() + ".html", "wb") as f:
            f.write(file)
        time.sleep(random.randint(1, 3))

def get_full_table(html):
    # get the table which is some child to "discovery-result-results"
    # and construct a pd df
    # html = requests.get(link).content
    content = soup.BeautifulSoup(html, "html.parser")

    # get the links in the table as well instead of the text

    table = content.find("table")
    df = pd.read_html(StringIO(str(table)), extract_links="body")[0] 

    print(df)
    df["Date"] = df["Date"].apply(lambda x: x[0])
    df["View"] = df["View"].apply(lambda x: x[1])
    df["Title"] = df["Title"].apply(lambda x: x[0])
    df['Type'] = df['Type'].apply(lambda x: x[0])
    df['Members'] = df['Members'].apply(lambda x: x[0])

    #rename link
    df.rename(columns={"View": "Meta_Link"}, inplace=True)

    # append this before the link
    pre = "https://eparlib.nic.in/"
    df["Meta_Link"] = pre + df["Meta_Link"]

    return df

# Open each link and find link with class btn-primary. get href

def get_pdf_meta_link(html):
    try:
        body = soup.BeautifulSoup(html, "html.parser")

        # get file name in table under parent div with class panel-info
        table = body.select_one("div.panel-info table td[headers=t1] a")
        link = table["href"]
        name = table.text 

        # get metadata in table with class itemDisplayTable
        table = body.select("table.itemDisplayTable tr")
        metatable = {i.select_one("td.metadataFieldLabel").text:
                    i.select_one("td.metadataFieldValue").text for i in table}
        return name, link, metatable

    except Exception:
        return ("", "", {})



def read_html_extract_tables(dir):
    dfParent = pd.read_csv("../data/sabha-debates-individual-documentation-links-meta.csv")
    df = pd.DataFrame(columns=["PDF_Name", "PDF_Link", "Meta", "Index"])

    # create a new csv file
    # df.to_csv("../data/sabha-debates-individual-documentation-links-meta.csv", mode="w", index=False)
    
    # make a new list of files
    # that dont exist in the df
    listFiles = os.listdir(dir)
    # if file in dfParent dont, remove
    listFiles = [x for x in listFiles if int(x.split(".")[0]) not in dfParent["Index"].values] 
    print(len(listFiles))   

    # read all files from dir
    for i, file in enumerate(listFiles):
        if file.endswith(".html"):
            path = os.path.join(dir, file)

            # check if file.strip(".")[0] is in index in df
            # if not, append
            # if yes, skip

            # get html content
            html = open(path, "r").read()
            
            # read html
            PDF_Name, PDF_Link, Meta = get_pdf_meta_link(html)

            # append to dataframe
            df = df._append({"PDF_Name": PDF_Name, "PDF_Link": PDF_Link, "Meta": Meta, "Index": file.split(".")[0]}, ignore_index=True)

            
            if i % 1000 == 0 and i != 0:
                print(i)
                # append to csv
                df.to_csv("../data/sabha-debates-individual-documentation-links-meta.csv", mode="a", index=False, header=False)
                df = pd.DataFrame(columns=["PDF_Name", "PDF_Link", "Meta", "Index"])

    return df


```

```{python}
import tqdm
# df = read_html_extract_tables("../data/htmlbkup/part2/")

#hardcode an index, drop unnamed columns
# 

# df = getFullTable(OriginalLink)
# df.to_csv("../data/sabha-debates-individual-documentation-links.csv")

# multi thread this
# do this
with ThreadPoolExecutor() as executor:
    df["PDF_Name"], df["PDF_Link"], df["Meta"] = zip(*executor.map(get_pdf_link, df["Meta_Link"]))


# split into smaller

df.to_csv("../data/sabha/sabha-debates-individual-documentation-links.csv")

```

# download orginal 

```{python}
import time 
import random
import pandas as pd
import requests

# df = pd.read_csv("../data/sabha-debates-individual-documentation-links.csv")

# # drop before 116710
# df = df[df["Index"] > 116710]
# with ThreadPoolExecutor() as executor:
#     for x, i in tqdm(zip(df["Meta_Link"], df["Index"])):
#         # download html
#         executor.submit(download_html, x, i)

# # open all files
# parse into a df using
# read_html_extract_tables("../data/htmlbkup/part2/meta/")

# remove duplicates
# df = pd.read_csv("../data/sabha-debates-individual-documentation-links.csv")
# df = df.drop_duplicates(subset=["Index"], keep="first")
# df.to_csv("../data/sabha-debates-individual-documentation-links.csv")


```

# count
```{python}
# in Date, get year (last 4 chars)

df["Year"] = df["Date"].apply(lambda x: x[-4:])

# group by year and print count per year
# display without truncating
pd.set_option("display.max_rows", None, "display.max_columns", None)
df.groupby("Year").count()

```

```{python}
import bs4 as soup
import random
import ast 
import shortuuid
import pandas as pd
# open a random html

def cleanupTable(s):
    # remove "\xa0"
    s = s.replace("\\xa0", " ")
    return s

def cleanupKeys(s):
    # remove colens 
    s = {k.replace(": ", " ").strip().replace(" ", "_"): v for  k,v in s.items()}
    return s

def splitCapital(s):
    # read words
    # if word contains two capital letters, split and insert a comma
    texts = s.split(' ')
    for i,t in enumerate(texts):
        for c in t:
            if c.isupper() and t.index(c) != 0:
                # push into a position inbetween in the arry
                temp = t[:t.index(c)] + ", " + t[t.index(c):]
                texts[i] = temp
    return " ".join(texts)

df = pd.read_csv("../data/loksabha/sabha-debates-individual-documentation-links-joined.csv")
df.dropna(subset="Meta", inplace=True)

df["Meta"] = df["Meta"].apply(lambda x: cleanupTable(x))

df["Meta"] = df["Meta"].apply(lambda x: ast.literal_eval(x))

df["Meta"] = df["Meta"].apply(lambda x: cleanupKeys(x))


# get keys in the dictionary
df["Participants"] = df["Meta"].apply(lambda x: splitCapital(x['Members'] if "Members" in x.keys() else ""))
df["Lok_Sabha_Number"] = df["Meta"].apply(lambda x: x["Lok_Sabha_Number"] if "Lok_Sabha_Number" in x.keys() else "")
df['Session_Number'] = df["Meta"].apply(lambda x: x["Session_Number"] if "Session_Number" in x.keys() else "")
df.rename(columns={"Category": "Type"}, inplace=True)
df['Type'] = df["Meta"].apply(lambda x: x["Debate"] if "Debate" in x.keys() else "")

# keep only unique indixes
df.drop_duplicates(subset=["Index"], keep="first", inplace=True)


# parse as date 
df["Date"] = df["Date"].apply(lambda x: pd.to_datetime(x))

df.sort_values("Date", inplace=True, ascending=False)

# create new index 
df.reset_index(inplace=True, drop=True)
df['Index'] = df.index

# set index of df
df.drop(columns=["Meta"], inplace=True)

df.drop(columns="Index")

# apply uuid to every row
df["UUID"] = ''
df['UUID'] = df["UUID"].apply(lambda x: shortuuid.uuid())
df.to_csv("../data/loksabha/sabha-debates-individual-documentation-flattened.csv", index=False)

```

```{python}
import pandas as pd

df = pd.read_csv("../data/loksabha/sabha-debates-individual-documentation-flattened.csv")

# get newest date entry 
df.sort_values("Date", inplace=True, ascending=False)
df.head(1)
# pd.set_option("display.max_colwidth", 1000)
# df["Text"][866]
```

```{python}
import pandas as pd

df = pd.read_csv("../data/loksabha/sabha-english-documentation-links.csv")

# count records year by year
df["Year"] = df["Date"].apply(lambda x: x[-4:])
pd.set_option("display.max_rows", None, "display.max_columns", None)
df.groupby("Year").count()
```